{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirakl Technical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "On an e-commerce site, it is crucial that products are categorized correctly. Incorrect categorization can lead to products being invisible to buyers and create a poor user experience, especially when large quantities of products are misplaced in the wrong category.\n",
    "\n",
    "<u>Guidelines:</u>\n",
    "- Analyze the data and describe some possible classification algorithms for such a dataset.\n",
    "- Choose and implement at least one of these algorithms using the training set (data_train.csv) and perform a prediction on the provided test set (data_test.csv). Specify the chosen metrics. Explain your steps.\n",
    "- Conduct a complete analysis of the obtained results.\n",
    "- Do not use the category hierarchy as an explanatory variable.\n",
    "\n",
    "<u>Objectives:</u>\n",
    "- A structured and concise presentation\n",
    "- Insightful data analysis and relevant modeling\n",
    "\n",
    "<u>Summary:</u>\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Data Preprocessing and Dimensionality Reduction Techniques\n",
    "3. Evaluation of Classification Algorithms and Linear Models\n",
    "4. Final Model Performance Evaluation and Next Steps\n",
    "5. Analysis of Sibling Misclassifications and Parent Category Performance\n",
    "6. Conclusion\n",
    "\n",
    "<u>Instructions:</u>\n",
    "- Data files must be stored in a \"data\" directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "<u>Dataset Size:</u> \n",
    "- The dataset contains 241,483 entries, which is reasonable for applying simple models with fewer parameters that should suffice for effective modeling.\n",
    "  \n",
    "<u>Features:</u>\n",
    "- All 128 features are numerical and of type float, with no missing values. This reduces the need for complex preprocessing and extensive feature engineering.\n",
    "- High dimensionality due to the 128 features suggests that dimensionality reduction could be beneficial before applying certain algorithms.\n",
    "    \n",
    "<u>Feature Correlation:</u>\n",
    "- The features are uncorrelated, indicating that simple linear models might perform well. However, potential interactions between features should not be dismissed.\n",
    "    \n",
    "<u>Product Categories:</u> \n",
    "- There are 101 product categories, which are identical in both the train and test sets.\n",
    "    \n",
    "<u>Category Distribution:</u>\n",
    "- The distribution is reasonably similar between the train and test sets, but the problem is highly imbalanced, with up to a 10x difference in category occurrences.\n",
    "- In order to avoid bias towards the majority classes, techniques such like class weighting, undersampling or oversampling like SMOTE (Synthetic Minority Over-sampling Technique) can be considered.\n",
    "\n",
    "<u>Evaluation Metric:</u>\n",
    "- Given the significant class imbalance, the F1-score is an appropriate evaluation metric. It balances precision and recall, making it effective in situations where the cost of false positives and false negatives are high, as in this case.\n",
    "\n",
    "<u>Remarks:</u> \n",
    "- Automatic EDA tools like \"ydata_profiling\" demand significant computational time, particularly for large or high-dimensional datasets like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path, PosixPath\n",
    "\n",
    "data_path : PosixPath = Path.cwd() / \"data\"\n",
    "\n",
    "category_parent_df : pd.DataFrame = pd.read_csv(data_path / \"category_parent.csv\")\n",
    "test_df : pd.DataFrame = pd.read_csv(data_path / \"data_test.csv.gz\", compression='gzip')\n",
    "train_df : pd.DataFrame = pd.read_csv(data_path / \"data_train.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    def print_df_summary(df, name):\n",
    "        print(f\"Summary for {name}:\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Data Types Count:\\n{df.dtypes.value_counts()}\")\n",
    "        print(f\"Max Missing Values (isna): {df.isna().sum(axis=0).max()}\")\n",
    "        print(f\"Max Missing Values (isnull): {df.isnull().sum(axis=0).max()}\\n\")\n",
    "    \n",
    "    print_df_summary(train_df, 'Train DataFrame')\n",
    "    print_df_summary(test_df, 'Test DataFrame')\n",
    "    \n",
    "    train_categories = set(train_df.category_id)\n",
    "    test_categories = set(test_df.category_id)\n",
    "    \n",
    "    assert train_categories == test_categories, \"Mismatch in categories between train and test DataFrames\"\n",
    "    print(\"Category consistency check passed: Train and Test categories are the same.\")\n",
    "\n",
    "def plot_correlation_matrix(df: pd.DataFrame, feature_start_col: int = 2) -> None:\n",
    "    corr_matrix = df.iloc[:, feature_start_col:].corr()\n",
    "    \n",
    "    max_corr = corr_matrix.abs().replace(1.0, None).max().max()\n",
    "    print(\"Maximum of correlation: \", max_corr)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1, annot=False, fmt='.2f', square=True)\n",
    "    plt.title(\"Correlation Matrix of Features\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_distribution_comparison(train_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    # Calculate normalized class distributions\n",
    "    train_class_counts = train_df['category_id'].value_counts(normalize=True)\n",
    "    test_class_counts = test_df['category_id'].value_counts(normalize=True)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    melted_class_distribution = (\n",
    "        pd.DataFrame({\n",
    "            'category': train_class_counts.index,\n",
    "            'train_percentage': train_class_counts.values * 100,\n",
    "            'test_percentage': test_class_counts.reindex(train_class_counts.index, fill_value=0).values * 100\n",
    "        })\n",
    "        .sort_values(by='train_percentage', ascending=False)\n",
    "        .melt(id_vars='category', \n",
    "              value_vars=['train_percentage', 'test_percentage'],\n",
    "              var_name='Set', value_name='Percentage')\n",
    "    )\n",
    "    \n",
    "    # Plot the class distribution comparison\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(x='category', y='Percentage', hue='Set', data=melted_class_distribution, palette={'train_percentage': 'blue', 'test_percentage': 'orange'})\n",
    "    plt.xlabel('Category ID')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Normalized Class Distribution Comparison Between Train and Test Set')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(title='Data Set')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(train_df, test_df)\n",
    "plot_correlation_matrix(train_df)\n",
    "plot_class_distribution_comparison(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Dimensionality Reduction Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "<u>Outliers:</u>\n",
    "- Rows with more than 10% outlier features (z-score absolute threshold > 3) were removed, accounting for 2.91% of the training dataset.\n",
    "    Analysis of row removal per category shows that no more than 10% of the data was removed for any category.\n",
    "\n",
    "<u>Train, Validation & Test Split:</u>\n",
    "- The training dataset was split into training and validation sets with a 0.8/0.2 ratio, ensuring stratification to maintain category distribution and random shuffling to prevent order bias.\n",
    "\n",
    "<u>Scaling:</u>\n",
    "- Standardization is applied to ensure features are on a similar scale, which helps improve model performance and convergence.\n",
    "\n",
    "<u>Dimensionality Reduction:</u>\n",
    "- PCA: Initial PCA suggests features are nearly orthogonal, requiring 120 features to explain 95% of the variance, with similar contributions from each feature.\n",
    "    LassoCV: Removed 2 features, aiding in reducing overfitting and improving generalization.\n",
    "\n",
    "<u>Feature Interaction:</u>\n",
    "- No polynomial features were created due to high dimensionality. Decision trees and ensemble methods like Random Forests and Gradient Boosting capture feature interactions implicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "\n",
    "RANDOM_SEED : int = 42\n",
    "TRAIN_TEST_SPLIT: int = 0.2\n",
    "\n",
    "def split_labels_from_features(df: pd.DataFrame):\n",
    "    X = df.iloc[:, 2:]  # Features\n",
    "    y = df[\"category_id\"]  # Labels\n",
    "    return X, y\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, feature_columns: list, threshold: float, max_outliers_per_row: int) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    # Calculate z-scores for the specified feature columns & determine outliers\n",
    "    z_scores = df[feature_columns].apply(zscore)\n",
    "    outliers = (z_scores > threshold) | (z_scores < -threshold)\n",
    "    \n",
    "    # Count the number of outliers per row & filter rows with excessive outliers\n",
    "    outliers_per_row = outliers.sum(axis=1) \n",
    "    filtered_df = df[outliers_per_row <= max_outliers_per_row]\n",
    "    return filtered_df, outliers_per_row\n",
    "\n",
    "def scale_features(X_train: pd.DataFrame, X: pd.DataFrame, scaler) -> tuple:\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    return X_train_scaled, X_scaled\n",
    "\n",
    "def select_features_with_lasso(X_train_scaled: np.ndarray, y_train_encoded: np.ndarray, X_val_scaled: np.ndarray, X_test_scaled: np.ndarray) -> tuple:\n",
    "\n",
    "    lasso_cv = LassoCV(cv=5, random_state=RANDOM_SEED)\n",
    "    lasso_cv.fit(X_train_scaled, y_train_encoded)\n",
    "    selected_features_mask = lasso_cv.coef_ != 0\n",
    "    \n",
    "    X_train_selected = X_train_scaled[:, selected_features_mask]\n",
    "    X_val_selected = X_val_scaled[:, selected_features_mask]\n",
    "    X_test_selected = X_test_scaled[:, selected_features_mask]\n",
    "\n",
    "    return X_train_selected, X_val_selected, X_test_selected, selected_features_mask\n",
    "\n",
    "def print_summary(original_df: pd.DataFrame, filtered_df: pd.DataFrame, selected_features_mask: np.ndarray, X_train: pd.DataFrame):\n",
    "    # Dataset shape information\n",
    "    original_size = original_df.shape\n",
    "    filtered_size = filtered_df.shape\n",
    "    rows_removed = original_size[0] - filtered_size[0]\n",
    "    \n",
    "    print(f\"Original dataset size: {original_size[0]} rows x {original_size[1]} columns\")\n",
    "    print(f\"Filtered dataset size: {filtered_size[0]} rows x {filtered_size[1]} columns\")\n",
    "    print(f\"Number of rows removed due to outliers: {rows_removed} ({100 * rows_removed / original_size[0]:.2f}%)\")\n",
    "    \n",
    "    # Feature selection information\n",
    "    num_selected_features = selected_features_mask.sum()\n",
    "    \n",
    "    print(f'Number of selected features after LassoCV: {num_selected_features}')\n",
    "\n",
    "def plot_outliers_distribution(outliers_per_row):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(outliers_per_row, kde=True, bins=30, color='skyblue')\n",
    "    plt.title('Distribution of Outliers per Row', fontsize=16)\n",
    "    plt.xlabel('Number of Outliers per Row', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def plot_category_removal_percentage(original_df: pd.DataFrame, filtered_df: pd.DataFrame):\n",
    "    initial_category_counts = original_df[\"category_id\"].value_counts()\n",
    "    filtered_category_counts = filtered_df[\"category_id\"].value_counts()\n",
    "    \n",
    "    # Align counts\n",
    "    aligned_counts = pd.concat([initial_category_counts, filtered_category_counts], axis=1, keys=['Before', 'After']).fillna(0)\n",
    "    aligned_counts['% Removed'] = (aligned_counts['Before'] - aligned_counts['After']) / aligned_counts['Before'] * 100\n",
    "    aligned_counts_sorted = aligned_counts.sort_values('% Removed', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=aligned_counts_sorted.index, y=aligned_counts_sorted['% Removed'], palette='coolwarm')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    plt.title('Percentage of Rows Removed per Product Category', fontsize=16)\n",
    "    plt.xlabel('Product Category', fontsize=14)\n",
    "    plt.ylabel('% of Rows Removed', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_SCORE_OUTLIERS_THRESHOLD: int = 3\n",
    "MAX_OUTLIERS_PER_ROW: int = 10\n",
    "\n",
    "# outliers\n",
    "feature_columns = train_df.columns[2:]\n",
    "filtered_train_df, outliers_per_row = remove_outliers(train_df, feature_columns, Z_SCORE_OUTLIERS_THRESHOLD, MAX_OUTLIERS_PER_ROW)\n",
    "\n",
    "# split the data\n",
    "X_test, y_test = split_labels_from_features(test_df)\n",
    "X, y = split_labels_from_features(filtered_train_df)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TRAIN_TEST_SPLIT, shuffle=True, stratify=y, random_state=RANDOM_SEED)\n",
    "\n",
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()  # or RobustScaler()\n",
    "X_train_scaled, X_val_scaled  = scale_features(X_train, X_val, scaler)\n",
    "X_train_scaled, X_test_scaled = scale_features(X_train, X_test, scaler)\n",
    "\n",
    "# features selection with Lasso\n",
    "X_train_selected, X_val_selected, X_test_selected, selected_features_mask = select_features_with_lasso(X_train_scaled, y_train_encoded, X_val_scaled, X_test_scaled)\n",
    "\n",
    "print_summary(train_df, filtered_train_df, selected_features_mask, X_train)\n",
    "plot_outliers_distribution(outliers_per_row)\n",
    "plot_category_removal_percentage(train_df, filtered_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation of Classification Algorithms and Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "<u>Some relevant classification Algorithms:</u>\n",
    "- Logistic Regression: A straightforward linear classifier that's efficient for high-dimensional data and handles class imbalance with class weighting.\n",
    "- Ridge Classifier: An extension of Logistic Regression with L2 regularization, helping to prevent overfitting in high-dimensional spaces.\n",
    "- RandomForest Classifier: An ensemble method that combines multiple decision trees, reducing overfitting and performing well with high-dimensional data.\n",
    "- XGBoost/LightGbm: A powerful gradient boosting algorithm that builds trees sequentially and includes parameters for addressing class imbalance.\n",
    "- Support Vector Machine (SVM): Effective for high-dimensional data, with the ability to capture complex relationships using different kernels, though it requires careful tuning for class imbalance.\n",
    "\n",
    "<u>Linear models as a baseline:</u>\n",
    "- High Dimensional Uncorrelated Features: The simplicity and interpretability of linear models are beneficial in this context.\n",
    "- Computational Efficiency: Linear models are computationally efficient and quick to train.\n",
    "- Benchmark for More Complex Models: If more complex models significantly outperform linear models, it indicates that there may be non-linear relationships or interactions that the linear model cannot capture\n",
    "\n",
    "<u>Performance Analysis of linear models:</u>\n",
    "- RidgeClassifier:\n",
    "    - Performs fairly consistently across training, validation, and test datasets.\n",
    "    - Strengths: Balances between training and validation sets, indicating good generalization.\n",
    "    - Weaknesses: Slightly lower performance on the test set compared to Logistic Regression.\n",
    "\n",
    "- LogisticRegression:\n",
    "    - Shows better performance on the training data and maintains relatively good performance on validation and test datasets.\n",
    "    - Strengths: Higher F1-scores compared to RidgeClassifier, indicating better overall performance.\n",
    "    - Weaknesses: Performance drop from training to test set, suggesting some level of overfitting.\n",
    "\n",
    "- F1-score vs. Support Percentage:\n",
    "    - Helps identify categories that may be misclassified due to low occurrence, and allows for performance comparison across categories (see below).\n",
    "    - Highlights that some frequently occurring classes are poorly predicted, while others with fewer occurrences are well predicted. This suggests a possible difference in data quality between categories. Consider removing more outliers or applying oversampling to address this issue.\n",
    "    - Examples:\n",
    "        - Category 9821 with more than 3% occurence and a f1-score of 0.62\n",
    "        - Category 2824 with 0.5% occurence and a f1-score of 0.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "\n",
    "def get_classification_report_df(model, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "    y_pred = model.predict(X)\n",
    "    return pd.DataFrame(classification_report(y, y_pred, output_dict=True)).transpose()\n",
    "\n",
    "def score_model(model, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame, \n",
    "                y_train: pd.Series, y_val: pd.Series, y_test: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # Predict and evaluate on training data\n",
    "    train_report_df = get_classification_report_df(model, X_train, y_train)\n",
    "    print(\"Training data classification report F1-score:\")\n",
    "    print(train_report_df.loc[:, \"f1-score\"].mean())\n",
    "\n",
    "    # Predict and evaluate on validation data\n",
    "    val_report_df = get_classification_report_df(model, X_val, y_val)\n",
    "    print(\"Validation data classification report F1-score:\")\n",
    "    print(val_report_df.loc[:, \"f1-score\"].mean())\n",
    "\n",
    "    # Predict and evaluate on test data\n",
    "    test_report_df = get_classification_report_df(model, X_test, y_test)\n",
    "    print(\"Test data classification report F1-score:\")\n",
    "    print(test_report_df.loc[:, \"f1-score\"].mean())\n",
    "    \n",
    "    return train_report_df, val_report_df, test_report_df\n",
    "\n",
    "def plot_f1_vs_support_percentage(df_report: pd.DataFrame) -> None:\n",
    "    # Filter out 'accuracy', 'macro avg', 'weighted avg'\n",
    "    df_report = df_report.iloc[:-3, :]\n",
    "    \n",
    "    # Calculate percentages for the support values\n",
    "    total_instances = df_report['support'].sum()\n",
    "    df_report['support_percentage'] = (df_report['support'] / total_instances) * 100\n",
    "    \n",
    "    # Plotting F1-score vs Support Percentage\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_report['support_percentage'], df_report['f1-score'], color='blue', alpha=0.7)\n",
    "    plt.title(\"Relationship between Support Percentage and F1-Score for Each Class\", fontsize=16)\n",
    "    plt.xlabel(\"Support (Percentage of Total Instances)\", fontsize=14)\n",
    "    plt.ylabel(\"F1-Score\", fontsize=14)\n",
    "    \n",
    "    # Optionally, annotate each point with the class label\n",
    "    for i, class_label in enumerate(df_report.index):\n",
    "        plt.annotate(class_label, (df_report['support_percentage'].iloc[i], df_report['f1-score'].iloc[i]),\n",
    "                     textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=9)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_clf = RidgeClassifier(class_weight=\"balanced\", random_state=RANDOM_SEED)\n",
    "ridge_clf.fit(X_train_selected, y_train)\n",
    "\n",
    "ridge_clf_train_report_df, ridge_clf_validation_report_df, ridge_clf_test_report_df = score_model(\n",
    "    ridge_clf, X_train_selected, X_val_selected, X_test_selected, y_train, y_val, y_test\n",
    ")\n",
    "plot_f1_vs_support_percentage(ridge_clf_validation_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(class_weight=\"balanced\", random_state=RANDOM_SEED)\n",
    "log_reg.fit(X_train_selected, y_train)\n",
    "\n",
    "log_reg_train_report_df, log_reg_validation_report_df, log_reg_test_report_df = score_model(\n",
    "    log_reg, X_train_selected, X_val_selected, X_test_selected, y_train, y_val, y_test\n",
    ")\n",
    "plot_f1_vs_support_percentage(log_reg_validation_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final Model Performance Evaluation and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "The following analysis focuses on the performance of the final model, which is the Logistic Regression trained on the entire training set. This model achieved the best results among those evaluated. Here's a summary of its performance and the recommended next steps for further improvement.\n",
    "\n",
    "<u>DecisionTreeClassifier:</u>\n",
    "- DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_SEED, max_depth=15) F1-score: 0.515 train, 0.383 validation & 0.355 test\n",
    "- Performs poorly on both validation and test datasets.\n",
    "- Strengths: Can model complex relationships but is heavily dependent on hyperparameters.\n",
    "- Weaknesses: High variance, significant drop in performance from training to validation and test datasets. Likely overfitting.\n",
    "\n",
    "<u>SVM & LightGBM:</u>\n",
    "- Training these models takes too long and requires excessive resources to prevent overfitting. Even though they remain clear challengers, they are set aside to stay pragmatic in this study.\n",
    "\n",
    "<u>Ensemble Methods</u>:\n",
    "- Combining models to leverage the strengths of each:\n",
    "    - Logistic regression and ridge classifier\n",
    "    - Logistic regression, ridge classifier and Decision Tree Classifier\n",
    "- However, as the comparison of F1 scores shows, both linear models make the same errors, so their combination does not improve performance. Adding the overfitted Decision Tree does not help either\n",
    "\n",
    "<u>Final Model: Logistic Regression on Full Training Set</u>\n",
    "- F1-Scores:\n",
    "    - Training Data: 0.66\n",
    "    - Validation Data: 0.64\n",
    "    - Test Data: 0.62\n",
    "- The model performs well across different datasets even if it might be slightly overfitting\n",
    "\n",
    "<u>Next Steps:</u>\n",
    "- Handling Imbalance: Investigate techniques like SMOTE or class weighting further to address class imbalance.\n",
    "- Hyperparameter Tuning & Cross-Validation: Fine-tune hyperparameters more extensively to find a balance between bias and variance.\n",
    "\n",
    "<u>Useful tools:</u>\n",
    "- Optuna: https://optuna.org/\n",
    "- MlfLow: https://mlflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_f1_scores(report_dict1: pd.DataFrame, report_dict2: pd.DataFrame, model1_name: str, model2_name: str) -> None:\n",
    "    # Convert classification report dictionaries to DataFrames\n",
    "    df1 = report_dict1.drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')\n",
    "    df2 = report_dict2.drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')\n",
    "    \n",
    "    # Concatenate F1-scores for all models\n",
    "    f1_df = pd.concat([df1[['f1-score']].rename(columns={'f1-score': model1_name}),\n",
    "                       df2[['f1-score']].rename(columns={'f1-score': model2_name})], axis=1)\n",
    "    \n",
    "    # Plot the comparison of F1-scores\n",
    "    f1_df.plot(kind='bar', figsize=(12, 6))\n",
    "    plt.title('F1-Score Comparison per Class')\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xlabel('Class')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_f1_scores(ridge_clf_validation_report_df, log_reg_validation_report_df, model1_name=\"Ridge Classifier\", model2_name=\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X_test, final_y_test = split_labels_from_features(test_df)\n",
    "final_X_train, final_y_train = split_labels_from_features(filtered_train_df)\n",
    "\n",
    "final_X_train, final_X_test = scale_features(final_X_train, final_X_test, scaler)\n",
    "\n",
    "final_X_test = final_X_test[:, selected_features_mask]\n",
    "final_X_train = final_X_train[:, selected_features_mask]\n",
    "\n",
    "final_log_reg = LogisticRegression(class_weight=\"balanced\", random_state=RANDOM_SEED)\n",
    "final_log_reg.fit(final_X_train, final_y_train)\n",
    "\n",
    "final_log_reg_train_report_df = get_classification_report_df(final_log_reg, final_X_train, final_y_train)\n",
    "final_log_reg_test_report_df = get_classification_report_df(final_log_reg, final_X_test, final_y_test)\n",
    "\n",
    "print(final_log_reg_train_report_df.loc[:, \"f1-score\"].mean())\n",
    "print(final_log_reg_test_report_df.loc[:, \"f1-score\"].mean())\n",
    "\n",
    "plot_f1_vs_support_percentage(final_log_reg_test_report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analysis of Sibling Misclassifications and Parent Category Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "The dataset category_parent.csv contains category-parent pairs, providing an opportunity to study sibling misclassifications.\n",
    "\n",
    "<u>Definitions:</u>\n",
    "- Sibling: Categories belonging to the same parent category.\n",
    "- The Fraction of Total Misclassifications: It is the percentage of mistakes that involve a specific parent category out of all the mistakes made by the model.\n",
    "- The Misclassification Ratio: It is the percentage of mistakes for a specific parent category out of all the instances that actually belong to that parent category.\n",
    "\n",
    "<u>Sibling Misclassification Rate:</u>\n",
    "- Rate: 22.64%\n",
    "- Implication: About 1 in 5 misclassifications occur between sibling categories, indicating the model's difficulty distinguishing between similar sibling categories.\n",
    "\n",
    "<u>High Misclassification Counts with Varying F1 Scores:</u>\n",
    "- Parent Category 9819: Approximately 32% of misclassifications occur within this parent category, but it has a good overall performance with an average F1 score of 0.63. While the category is generally well-predicted, developing distinguishing features for subcategories within Parent Category 9819 could improve classification accuracy further.\n",
    "- Parent Category 3066: This category has a similar misclassification ratio of 28.84%, but its average F1 score is significantly lower at 0.44. This suggests that not only is there a high misclassification rate, but the quality of predictions within this category is also poor.\n",
    "\n",
    "<u>Categories with Low Misclassification Ratios and Higher F1 Scores:</u>\n",
    "- Parent Category 2958 has the lowest misclassification ratio (11.40%) and a relatively high average F1 score (0.62). This suggests that this parent category is well-represented in the model, with fewer misclassifications and reasonably accurate predictions.\n",
    "- Parent Category 2891 has a low misclassification ratio (16.49%) and a decent average F1 score (0.58), indicating that it is relatively well classified with a lower proportion of errors.\n",
    "\n",
    "<u>Fractions of Total Misclassifications:</u>\n",
    "- Categories with higher fractions of total misclassifications (e.g., Parent Category 9819 with 4.26%) point to potentially significant issues in the modelâ€™s performance for these categories. This can help prioritize areas for further improvement.\n",
    "\n",
    "<u>Association Between F1 Score and Misclassification Ratio:</u>\n",
    "- There seems to be an inverse relationship between the misclassification ratio and the average F1 score. Higher misclassification ratios are often associated with lower average F1 scores, suggesting that improving the F1 score might help reduce the misclassification ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, DefaultDict, Tuple\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "CategoryDict = Dict[int, List[int]]\n",
    "\n",
    "def is_sibling_misclassification(true_label: int, predicted_label: int, category_to_parent: CategoryDict) -> bool:\n",
    "    return any(true_label in siblings and predicted_label in siblings for siblings in category_to_parent.values())\n",
    "\n",
    "def analyze_sibling_misclassifications(true_labels: List[int], predicted_labels: List[int], category_to_parent: CategoryDict) -> float:\n",
    "    sibling_misclassifications = sum(\n",
    "        1 for true_label, predicted_label in zip(true_labels, predicted_labels)\n",
    "        if true_label != predicted_label and is_sibling_misclassification(true_label, predicted_label, category_to_parent)\n",
    "    )\n",
    "    total_misclassifications = sum(\n",
    "        1 for true_label, predicted_label in zip(true_labels, predicted_labels)\n",
    "        if true_label != predicted_label\n",
    "    )\n",
    "    return sibling_misclassifications / total_misclassifications if total_misclassifications > 0 else 0\n",
    "\n",
    "def calculate_f1_scores(true_labels: List[int], predicted_labels: List[int], category_list: List[int]) -> Dict[int, float]:\n",
    "    f1_scores = {}\n",
    "    for category in category_list:\n",
    "        true_binary = [1 if label == category else 0 for label in true_labels]\n",
    "        predicted_binary = [1 if label == category else 0 for label in predicted_labels]\n",
    "        f1_scores[category] = f1_score(true_binary, predicted_binary, zero_division=0)\n",
    "    return f1_scores\n",
    "\n",
    "def count_sibling_misclassifications(true_labels: List[int], predicted_labels: List[int], categories_per_parent_id_dict: CategoryDict) -> Tuple[DefaultDict[int, int], DefaultDict[int, int]]:\n",
    "    misclassification_counts = defaultdict(int)\n",
    "    occurrence_counts = defaultdict(int)\n",
    "    \n",
    "    for true_label, predicted_label in zip(true_labels, predicted_labels):\n",
    "        if true_label != predicted_label and is_sibling_misclassification(true_label, predicted_label, categories_per_parent_id_dict):\n",
    "            parent = next((parent for parent, siblings in categories_per_parent_id_dict.items() if true_label in siblings or predicted_label in siblings), None)\n",
    "            if parent is not None:\n",
    "                misclassification_counts[parent] += 1\n",
    "        # Increment occurrences for each true label\n",
    "        parent = next((parent for parent, siblings in categories_per_parent_id_dict.items() if true_label in siblings), None)\n",
    "        if parent is not None:\n",
    "            occurrence_counts[parent] += 1\n",
    "    \n",
    "    return misclassification_counts, occurrence_counts\n",
    "\n",
    "def analyze_misclassified_parent_groups(true_labels: List[int], predicted_labels: List[int], categories_per_parent_id_dict: CategoryDict) -> None:\n",
    "    misclassification_counts, occurrence_counts = count_sibling_misclassifications(true_labels, predicted_labels, categories_per_parent_id_dict)\n",
    "    \n",
    "    all_categories = [cat for cats in categories_per_parent_id_dict.values() for cat in cats]\n",
    "    f1_scores = calculate_f1_scores(true_labels, predicted_labels, all_categories)\n",
    "    \n",
    "    print(\"Parent Category Misclassification Counts, Average F1 Scores, and Ratios:\")\n",
    "    for parent, count in misclassification_counts.items():\n",
    "        associated_categories = categories_per_parent_id_dict.get(parent, [])\n",
    "        associated_f1_scores = [f1_scores.get(cat, 0) for cat in associated_categories]\n",
    "        average_f1_score = sum(associated_f1_scores) / len(associated_f1_scores) if associated_f1_scores else 0\n",
    "        \n",
    "        # Calculate fraction of total misclassifications\n",
    "        total_misclassifications = len([1 for true_label, predicted_label in zip(true_labels, predicted_labels) if true_label != predicted_label])\n",
    "        fraction_misclassifications = count / total_misclassifications if total_misclassifications > 0 else 0\n",
    "        \n",
    "        # Calculate ratio of misclassifications to occurrences\n",
    "        occurrences = occurrence_counts.get(parent, 0)\n",
    "        misclassification_ratio = (count / occurrences * 100) if occurrences > 0 else 0\n",
    "        \n",
    "        print(f\"Parent Category {parent}: {count} misclassifications\")\n",
    "        print(f\"  Associated Categories: {associated_categories}\")\n",
    "        print(f\"  Average F1 Score: {average_f1_score:.2f}\")\n",
    "        print(f\"  Fraction of Total Misclassifications: {fraction_misclassifications:.2%}\")\n",
    "        print(f\"  Misclassification Ratio: {misclassification_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_per_parent_id_dict = (\n",
    "    category_parent_df\n",
    "    .loc[lambda df: df.category_id.isin(set(train_df.category_id))]\n",
    "    .assign(parent_id=lambda df: df.parent_id.astype(int),\n",
    "            category_id=lambda df: df.category_id.astype(int))\n",
    "    .groupby(\"parent_id\")\n",
    "    .aggregate({\"category_id\": [list]})\n",
    "    .to_dict()\n",
    "    [('category_id', 'list')]\n",
    ")\n",
    "\n",
    "predicted_labels = final_log_reg.predict(final_X_test)\n",
    "\n",
    "# Calculate sibling error rate\n",
    "sibling_error_rate = analyze_sibling_misclassifications(final_y_test, predicted_labels, categories_per_parent_id_dict)\n",
    "print(f\"Sibling Misclassification Rate: {sibling_error_rate:.2%}\")\n",
    "\n",
    "# Identify parent groups with high misclassification counts and ratios\n",
    "analyze_misclassified_parent_groups(final_y_test, predicted_labels, categories_per_parent_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Achievements:</u>\n",
    "- Model Performance: \n",
    "    - Logistic Regression demonstrated the highest performance, with F1-scores of 0.66 on the training set, 0.64 on the validation set, and 0.62 on the test set. This model strikes an excellent balance between performance and complexity due to its simplicity and interpretability.\n",
    "- Misclassification Insights:\n",
    "    - Identified parent categories with high internal misclassification ratios (e.g., 9819, 3066) and those with lower ratios and higher F1 scores (e.g., 2958, 2891). Addressing categories with high misclassification ratios can directly enhance customer experience and operational accuracy.\n",
    "\n",
    "<u>Next Steps:</u>\n",
    "- Feature Engineering:\n",
    "    - Create or refine features to better differentiate between similar categories, particularly for those with high misclassification rates.\n",
    "- Hyperparameter Tuning:\n",
    "    - Perform extensive hyperparameter tuning and cross-validation to enhance model performance and optimize the balance between bias and variance.\n",
    "- Explore Complex Models:\n",
    "    - Consider incorporating more complex models, such as LightGBM, to potentially improve classification performance further.\n",
    "Strategic Focus:\n",
    "    - Concentrate on categories with significant internal misclassification to gain more precise and actionable business insights, leading to improved decision-making and operational efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
